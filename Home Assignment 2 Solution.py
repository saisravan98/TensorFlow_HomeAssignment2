# -*- coding: utf-8 -*-
"""Welcome to Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb

### **Home Assignment 2**

Student Name: CHINTALA SAI SRAVAN

Student ID: 700773836

Question 1: Convolution Operations with Different Parameters

Task: Implement Convolution with Different Stride and Padding (10 points)

Write a Python script using NumPy and TensorFlow/Keras to perform convolution on a 5×5 input matrix using a 3×3 kernel with varying parameters.
"""

import numpy as np
import tensorflow as tf

# Define the 5x5 input matrix
input_matrix = np.array([
    [1, 2, 3, 4, 5],
    [6, 7, 8, 9, 10],
    [11, 12, 13, 14, 15],
    [16, 17, 18, 19, 20],
    [21, 22, 23, 24, 25]
], dtype=np.float32)

# Reshape input for TensorFlow (batch_size=1, height, width, channels)
input_tensor = input_matrix.reshape(1, 5, 5, 1)

# Define the 3x3 kernel
kernel = np.array([
    [0, 1, 0],
    [1, -4, 1],
    [0, 1, 0]
], dtype=np.float32)

# Reshape kernel for TensorFlow (height, width, input_channels, output_channels)
kernel_tensor = kernel.reshape(3, 3, 1, 1)

# Define a function to perform convolution
def perform_convolution(input_tensor, kernel_tensor, strides, padding):
    strides = [1, strides, strides, 1]  # Format for TensorFlow
    output = tf.nn.conv2d(input=input_tensor, filters=kernel_tensor, strides=strides, padding=padding)
    return output.numpy().squeeze()  # Convert output to NumPy and remove extra dimensions

# Perform convolution operations
results = {
    "Stride = 1, Padding = 'VALID'": perform_convolution(input_tensor, kernel_tensor, strides=1, padding='VALID'),
    "Stride = 1, Padding = 'SAME'": perform_convolution(input_tensor, kernel_tensor, strides=1, padding='SAME'),
    "Stride = 2, Padding = 'VALID'": perform_convolution(input_tensor, kernel_tensor, strides=2, padding='VALID'),
    "Stride = 2, Padding = 'SAME'": perform_convolution(input_tensor, kernel_tensor, strides=2, padding='SAME')
}

# Print the output feature maps
for description, result in results.items():
    print(description + ":\n", result, "\n")

"""Question 2: CNN Feature Extraction with Filters and Pooling

Task 1: Implement Edge Detection Using Convolution (15 points)

Write a Python script using NumPy and OpenCV (cv2) to apply edge detection on an image using a Sobel filter.
*   Load a grayscale image (you can use any sample image).
*   Apply the Sobel filter for edge detection in the x-direction and y-direction.
*   Display the original image and the filtered images.

Use the following Sobel filters:

	              -1    0     1
    Sobel X =   -2    0     2
                -1    0     1

                -1    -2    -1
    Sobel Y =   0     0     0
                1     2     1


"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tensorflow.keras import layers
import requests  # Import the requests library
from PIL import Image  # Import the Image class from Pillow
from io import BytesIO  # Import the BytesIO class


# Load image from URL (or upload your own image)
url = "https://images.pexels.com/photos/962312/pexels-photo-962312.jpeg"
response = requests.get(url)
image = Image.open(BytesIO(response.content)).convert('RGB')
image = image.resize((128, 128))  # Resize for simplicity


# Load a sample grayscale image
#image = cv2.imread('', cv2.IMREAD_GRAYSCALE)

# Define Sobel filters
sobel_x = np.array([[-1, 0, 1],
                    [-2, 0, 2],
                    [-1, 0, 1]])
sobel_y = np.array([[-1, -2, -1],
                    [0, 0, 0],
                    [1, 2, 1]])

# Apply Sobel filter in x and y directions
# Note: cv2.filter2D expects a grayscale image for single-channel convolution.
# Since you loaded an RGB image and converted it to 'RGB', the image has 3 channels.
# For simplicity, you might want to convert it to grayscale before applying the Sobel filters.
# If you intend to apply the filter channel-wise, you'll need a different approach or
# ensure the input to cv2.filter2D is a single-channel image.
# Assuming you want to work with a grayscale image for edge detection:
image_gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)


sobel_x_output = cv2.filter2D(image_gray, -1, sobel_x)
sobel_y_output = cv2.filter2D(image_gray, -1, sobel_y)

# Display results
plt.figure(figsize=(10, 10))
plt.subplot(1, 3, 1)
plt.title("Original Image")
plt.imshow(image, cmap='gray') # Displaying the original RGB image for context

plt.subplot(1, 3, 2)
plt.title("Sobel-X")
plt.imshow(sobel_x_output, cmap='gray')

plt.subplot(1, 3, 3)
plt.title("Sobel-Y")
plt.imshow(sobel_y_output, cmap='gray')

plt.show()

"""Task 2: Implement Max Pooling and Average Pooling (15 points)

Write a Python script using TensorFlow/Keras to demonstrate Max Pooling and Average Pooling.

•	Create a random 4x4 matrix as an input image.

•	Apply a 2x2 Max Pooling operation.

•	Apply a 2x2 Average Pooling operation.

•	Print the original matrix, max-pooled matrix, and average-pooled matrix.

"""

import numpy as np
import tensorflow as tf

# Create a random 4x4 matrix
# Convert the integer array to float32
input_matrix = np.random.randint(0, 10, (1, 4, 4, 1)).astype(np.float32) # Shape: (batch_size=1, height=4, width=4, channels=1)

# Define Max Pooling and Average Pooling layers
max_pooling = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='VALID')
average_pooling = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='VALID')

# Apply pooling operations
max_pooled_matrix = max_pooling(input_matrix).numpy().squeeze()
average_pooled_matrix = average_pooling(input_matrix).numpy().squeeze()

# Print the original and pooled matrices
print("Original Matrix:")
print(input_matrix.squeeze().astype(np.int16))  # Remove extra dimensions for better readability

print("\nMax-Pooled Matrix (2x2):")
print(max_pooled_matrix.astype(np.int16))

print("\nAverage-Pooled Matrix (2x2):")
print(average_pooled_matrix.astype(np.int16))

"""Question 3: Data Preprocessing - Standardization vs. Normalization

Task: Data preprocessing ensures that input data is in the right format for neural networks. You will explore standardization and normalization techniques on a dataset and analyze their impact.

1.	Load the Iris dataset from sklearn.datasets.
2.	Implement Min-Max Normalization on all numeric features and print the transformed dataset.
3.	Implement Z-score Standardization and compare it with normalization by visualizing distributions using histograms.
4.	Train a simple Logistic Regression model before and after applying these transformations and compare the accuracy.
5.	Explain in which scenarios normalization vs. standardization is preferable for deep learning.

Hint: Use MinMaxScaler, StandardScaler, and train_test_split from sklearn.preprocessing and sklearn.model_selection.

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Step 1: Load the Iris dataset
iris = load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)
target = iris.target

# Step 2: Min-Max Normalization
scaler_minmax = MinMaxScaler()
normalized_data = scaler_minmax.fit_transform(data)

# Print normalized dataset
print("Normalized Dataset:")
print(pd.DataFrame(normalized_data, columns=iris.feature_names).head())

# Step 3: Z-score Standardization
scaler_zscore = StandardScaler()
standardized_data = scaler_zscore.fit_transform(data)

# Compare distributions using histograms
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(normalized_data, bins=20, label=iris.feature_names)
plt.title("Normalized Data Distribution")
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(standardized_data, bins=20, label=iris.feature_names)
plt.title("Standardized Data Distribution")
plt.legend()

plt.tight_layout()
plt.show()

# Step 4: Train Logistic Regression and Compare
def train_and_evaluate(data, target):
    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)
    model = LogisticRegression(max_iter=200)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# Accuracy without preprocessing
accuracy_original = train_and_evaluate(data, target)

# Accuracy with normalization
accuracy_normalized = train_and_evaluate(normalized_data, target)

# Accuracy with standardization
accuracy_standardized = train_and_evaluate(standardized_data, target)

print(f"Accuracy without preprocessing: {accuracy_original:.2f}")
print(f"Accuracy with normalization: {accuracy_normalized:.2f}")
print(f"Accuracy with standardization: {accuracy_standardized:.2f}")

# Step 5: Explain scenarios for normalization vs standardization
print("""
Normalization is preferable when features have different ranges, and you want all values between 0 and 1 (e.g., image pixel intensities).
Standardization is preferable when the dataset has features with varying scales, but you want a standard normal distribution (mean = 0, std = 1), such as in SVMs, PCA, or many deep learning models.
""")